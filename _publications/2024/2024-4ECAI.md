---
title:          "Inside the Black Box: Detecting Data Leakage in Pre-trained Language Encoders"
date:           2024-01-04
selected:       false
#type:           publication
#tags:           ["# continual learning", "# few-shot learning"]
pub:            "ECAI 2024"
# pub_pre:        "Submitted to "
# pub_post:       'Under review.'
# pub_last:       ' <span class="badge badge-pill badge-publication badge-success">Spotlight</span>'
# pub_date:       "2025"
semantic_scholar_id: 15b9d049822634496361f774d39545bb346ecc65  # use this to retrieve citation count
abstract: >-
  Despite being prevalent in the general field of Natural Language Processing (NLP), pre-trained language models inherently carry privacy and copyright concerns due to their nature of training on large-scale web-scraped data. In this paper, we pioneer a systematic exploration of such risks associated with pre-trained language encoders, specifically focusing on the membership leakage of pre-training data exposed through downstream models adapted from pre-trained language encoders-an aspect largely overlooked in existing literature. Our study encompasses comprehensive experiments across four types of pre-trained encoder architectures, three representative downstream tasks, and five benchmark datasets. Intriguingly, our evaluations reveal, ...
cover:          /assets/images/covers/2024-4ECAI.png
authors:
  - Yuan Xin
  - Zheng Li
  - Ning Yu
  - Dingfan Chen#
  - Mario Fritz
  - Michael Backes
  - Yang Zhang

links:
  Paper: https://arxiv.org/pdf/2408.11046
---
