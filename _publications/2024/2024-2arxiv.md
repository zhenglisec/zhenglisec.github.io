---
title:          "Model Hijacking Attack in Federated Learning"
date:           2024-01-02
selected:       false
#type:           publication
#tags:           ["# continual learning", "# few-shot learning"]
pub:            "arxiv 2024"
# pub_pre:        "Submitted to "
# pub_post:       'Under review.'
# pub_last:       ' <span class="badge badge-pill badge-publication badge-success">Spotlight</span>'
# pub_date:       "2025"
semantic_scholar_id: a9fedafe9e4afc7c16dd4c1773c4cb8c5a5cb5dc  # use this to retrieve citation count
abstract: >-
  Machine learning (ML), driven by prominent paradigms such as centralized and federated learning, has made significant progress in various critical applications ranging from autonomous driving to face recognition. However, its remarkable success has been accompanied by various attacks. Recently, the model hijacking attack has shown that ML models can be hijacked to execute tasks different from their original tasks, which increases both accountability and parasitic computational risks. Nevertheless, thus far, this attack has only focused on centralized learning. In this work, we broaden the scope of this attack to the federated learning domain, where multiple clients collaboratively train a global model without sharing their data. Specifically, we present HijackFL,  ...
cover:          /assets/images/covers/2024-2arxiv.png
authors:
  - Zheng Li
  - Siyuan Wu
  - Ruichuan Chen
  - Paarijaat Aditya
  - Istemi Ekin Akkus
  - Manohar Vanga
  - Min Zhang
  - Hao Li
  - Yang Zhang
links:
  Paper: https://arxiv.org/pdf/2408.02131
---
