---
title:          "SecurityNet: Assessing Machine Learning Vulnerabilities on Public Models"
date:           2024-01-05
selected:       false
#type:           publication
#tags:           ["# continual learning", "# few-shot learning"]
pub:            "USENIX Security 2024"
# pub_pre:        "Submitted to "
# pub_post:       'Under review.'
# pub_last:       ' <span class="badge badge-pill badge-publication badge-success">Spotlight</span>'
# pub_date:       "2025"
semantic_scholar_id: 468a78d431be0d6290bb3007e6920e15761b751e  # use this to retrieve citation count
abstract: >-
  While advanced machine learning (ML) models are deployed in numerous real-world applications, previous works demonstrate these models have security and privacy vulnerabilities. Various empirical research has been done in this field. However, most of the experiments are performed on target ML models trained by the security researchers themselves. Due to the high computational resource requirement for training advanced models with complex architectures, researchers generally choose to train a few target models using relatively simple architectures on typical experiment datasets. We argue that to understand ML models' vulnerabilities comprehensively, experiments should be performed on a large set of models trained with various purposes (not just the purpose of evaluating ML attacks and defenses). ...
cover:          /assets/images/covers/2024-5USENIX.png
authors:
  - Boyang Zhang
  - Zheng Li
  - Ziqing Yang
  - Xinlei He
  - Michael Backes
  - Mario Fritz
  - Yang Zhang

links:
  Paper: https://www.usenix.org/system/files/usenixsecurity24-zhang-boyang.pdf
  Code: https://github.com/SecurityNet-Research/SecurityNet
---
