---
title:          "Membership Inference Attacks Against In-Context Learning"
date:           2024-01-08
selected:       false
#type:           publication
#tags:           ["# continual learning", "# few-shot learning"]
pub:            "CCS 2024"
# pub_pre:        "Submitted to "
# pub_post:       'Under review.'
# pub_last:       ' <span class="badge badge-pill badge-publication badge-success">Spotlight</span>'
# pub_date:       "2025"
semantic_scholar_id: 6e8e815f6f0c5370e86651ed959f1c77657d607b  # use this to retrieve citation count
abstract: >-
  Adapting Large Language Models (LLMs) to specific tasks introduces concerns about computational efficiency, prompting an exploration of efficient methods such as In-Context Learning (ICL). However, the vulnerability of ICL to privacy attacks under realistic assumptions remains largely unexplored. In this work, we present the first membership inference attack tailored for ICL, relying solely on generated texts without their associated probabilities. We propose four attack strategies tailored to various constrained scenarios and conduct extensive experiments on four popular large language models. ...
cover:          /assets/images/covers/2024-8CCS.png
authors:
  - Rui Wen
  - Zheng Li#
  - Michael Backes
  - Yang Zhang#

links:
  Paper: https://dl.acm.org/doi/abs/10.1145/3658644.3690306
---
