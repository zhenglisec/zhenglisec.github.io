---
title:          "BadMerging: Backdoor Attacks Against Model Merging"
date:           2024-01-06
selected:       false
#type:           publication
#tags:           ["# continual learning", "# few-shot learning"]
pub:            "CCS 2024"
# pub_pre:        "Submitted to "
# pub_post:       'Under review.'
# pub_last:       ' <span class="badge badge-pill badge-publication badge-success">Spotlight</span>'
# pub_date:       "2025"
semantic_scholar_id: daaff30d9c716cd5ce747774f2057eb248185615  # use this to retrieve citation count
abstract: >-
  Fine-tuning pre-trained models for downstream tasks has led to a proliferation of open-sourced task-specific models. Recently, Model Merging (MM) has emerged as an effective approach to facilitate knowledge transfer among these independently fine-tuned models. MM directly combines multiple fine-tuned task-specific models into a merged model without additional training, and the resulting model shows enhanced capabilities in multiple tasks. Although MM provides great utility, it may come with security risks because an adversary can exploit MM to affect multiple downstream tasks. However, the security risks of MM have barely been studied. In this paper, we first find that MM, as a new learning paradigm, introduces unique challenges for existing backdoor attacks due to the merging process. ...
cover:          /assets/images/covers/2024-6CCS.png
authors:
  - Jinghuai Zhang#
  - Jianfeng Chi
  - Zheng Li
  - Kunlin Cai
  - Yang Zhang
  - Yuan Tian#

links:
  Paper: https://dl.acm.org/doi/pdf/10.1145/3658644.3690284
  Code: https://github.com/jzhang538/BadMerging
---
