---
title:          "ModScan: Measuring Stereotypical Bias in Large Vision-Language Models from Vision and Language Modalities"
date:           2024-01-09
selected:       false
#type:           publication
#tags:           ["# continual learning", "# few-shot learning"]
pub:            "EMNLP 2024"
# pub_pre:        "Submitted to "
# pub_post:       'Under review.'
# pub_last:       ' <span class="badge badge-pill badge-publication badge-success">Spotlight</span>'
# pub_date:       "2025"
semantic_scholar_id: c1814f8d52c8b25f062115bdb92735d8ca93b7b9  # use this to retrieve citation count
abstract: >-
  Large vision-language models (LVLMs) have been rapidly developed and widely used in various ffelds, but the (potential) stereotypical bias in the model is largely unexplored. In this study, we present a pioneering measurement framework, ModSCAN, to SCAN the stereotypical bias within LVLMs from both vision and language Modalities. ModSCAN examines stereotypical biases with respect to two typical stereotypical attributes (gender and race) across three kinds of scenarios: occupations, descriptors, and persona traits. ...
cover:          /assets/images/covers/2024-9EMNLP.png
authors:
  - Yukun Jiang
  - Zheng Li#
  - Xinyue Shen
  - Yugeng Liu
  - Michael Backes
  - Yang Zhang#

links:
  Paper: https://arxiv.org/pdf/2410.06967
---
