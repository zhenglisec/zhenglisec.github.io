---
title:          "Membership Inference Attacks Against Vision-Language Models"
date:           2025-01-02
selected:       false
#type:           publication
#tags:           ["# continual learning", "# few-shot learning"]
pub:            "USENIX Security 2025"
# pub_pre:        "Submitted to "
# pub_post:       'Under review.'
# pub_last:       ' <span class="badge badge-pill badge-publication badge-success">Spotlight</span>'
# pub_date:       "2025"
semantic_scholar_id: aa7d500086c0ad534dbe6a705003d794573d925a  # use this to retrieve citation count
abstract: >-
  Vision-Language Models (VLMs), built on pre-trained vision encoders and large language models (LLMs), have shown exceptional multi-modal understanding and dialog capabilities, positioning them as catalysts for the next technological revolution. However, while most VLM research focuses on enhancing multi-modal interaction, the risks of data misuse and leakage have been largely unexplored. This prompts the need for a comprehensive investigation of such risks in VLMs. ...
cover:          /assets/images/covers/2025-2Usenix.png
authors:
  - Yuke Hu
  - Zheng Li
  - Zhihao Liu
  - Yang Zhang
  - Zhan Qin#
  - Kui Ren
  - Chun Chen
links:
  Paper: https://www.usenix.org/system/files/conference/usenixsecurity25/sec25cycle1-prepub-977-hu-yuke.pdf
  Code: https://github.com/YukeHu/vlm_mia
---
