---
title:          "Membership Leakage in Label-Only Exposures"
date:           2021-01-01
selected:       true
#type:           publication
#tags:           ["# continual learning", "# few-shot learning"]
pub:            "CCS 2021"
# pub_pre:        "Submitted to "
# pub_post:       'Under review.'
# pub_last:       ' <span class="badge badge-pill badge-publication badge-success">Spotlight</span>'
# pub_date:       "2025"
semantic_scholar_id: 3ccf68128d78cb98d638b5b882d1be0fa3d8cb49  # use this to retrieve citation count
abstract: >-
  Machine learning (ML) has been widely adopted in various privacy-critical applications, e.g., face recognition and medical image analysis. However, recent research has shown that ML models are vulnerable to attacks against their training data. Membership inference is one major attack in this domain: Given a data sample and model, an adversary aims to determine whether the sample is part of the model's training set. Existing membership inference attacks leverage the confidence scores returned by the model as their inputs (score-based attacks). However, these attacks can be easily mitigated if the model only exposes the predicted label, i.e., the final model decision. ...
cover:          /assets/images/covers/2021-1CCS.png
authors:
  - Zheng Li
  - Yang Zhang#
links:
  Paper: https://dl.acm.org/doi/abs/10.1145/3460120.3484575
  Code: https://github.com/zhenglisec/Label-Only-MIA
---
